model

MaxViT(
  (conv_stem): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (layers): ModuleList(
    (0): Sequential(
      (0): Sequential(
        (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): SiLU()
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)
        (4): SqueezeExcitation(
          (gate): Sequential(
            (0): Reduce('b c h w -> b c', 'mean')
            (1): Linear(in_features=96, out_features=24, bias=False)
            (2): SiLU()
            (3): Linear(in_features=24, out_features=96, bias=False)
            (4): Sigmoid()
            (5): Rearrange('b c -> b c 1 1')
          )
        )
        (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): Rearrange('b d (x w1) (y w2) -> b x y w1 w2 d', w1=7, w2=7)
      (2): PreNormResidual(
        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (fn): Attention(
          (to_qkv): Linear(in_features=96, out_features=288, bias=False)
          (attend): Sequential(
            (0): Softmax(dim=-1)
            (1): Dropout(p=0.1, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=False)
            (1): Dropout(p=0.1, inplace=False)
          )
          (rel_pos_bias): Embedding(169, 3)
        )
      )
      (3): PreNormResidual(
        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (fn): FeedForward(
          (net): Sequential(
            (0): Linear(in_features=96, out_features=384, bias=True)
            (1): GELU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=384, out_features=96, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): Rearrange('b x y w1 w2 d -> b d (x w1) (y w2)')
      (5): Rearrange('b d (w1 x) (w2 y) -> b x y w1 w2 d', w1=7, w2=7)
      (6): PreNormResidual(
        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (fn): Attention(
          (to_qkv): Linear(in_features=96, out_features=288, bias=False)
          (attend): Sequential(
            (0): Softmax(dim=-1)
            (1): Dropout(p=0.1, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=False)
            (1): Dropout(p=0.1, inplace=False)
          )
          (rel_pos_bias): Embedding(169, 3)
        )
      )
      (7): PreNormResidual(
        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (fn): FeedForward(
          (net): Sequential(
            (0): Linear(in_features=96, out_features=384, bias=True)
            (1): GELU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=384, out_features=96, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (8): Rearrange('b x y w1 w2 d -> b d (w1 x) (w2 y)')
    )
    (1): Sequential(
      (0): MBConvResidual(
        (fn): Sequential(
          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU()
          (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
          (4): SqueezeExcitation(
            (gate): Sequential(
              (0): Reduce('b c h w -> b c', 'mean')
              (1): Linear(in_features=96, out_features=24, bias=False)
              (2): SiLU()
              (3): Linear(in_features=24, out_features=96, bias=False)
              (4): Sigmoid()
              (5): Rearrange('b c -> b c 1 1')
            )
          )
          (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
          (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (dropsample): Dropsample()
      )
      (1): Rearrange('b d (x w1) (y w2) -> b x y w1 w2 d', w1=7, w2=7)
      (2): PreNormResidual(
        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (fn): Attention(
          (to_qkv): Linear(in_features=96, out_features=288, bias=False)
          (attend): Sequential(
            (0): Softmax(dim=-1)
            (1): Dropout(p=0.1, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=False)
            (1): Dropout(p=0.1, inplace=False)
          )
          (rel_pos_bias): Embedding(169, 3)
        )
      )
      (3): PreNormResidual(
        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (fn): FeedForward(
          (net): Sequential(
            (0): Linear(in_features=96, out_features=384, bias=True)
            (1): GELU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=384, out_features=96, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): Rearrange('b x y w1 w2 d -> b d (x w1) (y w2)')
      (5): Rearrange('b d (w1 x) (w2 y) -> b x y w1 w2 d', w1=7, w2=7)
      (6): PreNormResidual(
        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (fn): Attention(
          (to_qkv): Linear(in_features=96, out_features=288, bias=False)
          (attend): Sequential(
            (0): Softmax(dim=-1)
            (1): Dropout(p=0.1, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=False)
            (1): Dropout(p=0.1, inplace=False)
          )
          (rel_pos_bias): Embedding(169, 3)
        )
      )
      (7): PreNormResidual(
        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (fn): FeedForward(
          (net): Sequential(
            (0): Linear(in_features=96, out_features=384, bias=True)
            (1): GELU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=384, out_features=96, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (8): Rearrange('b x y w1 w2 d -> b d (w1 x) (w2 y)')
    )
    (2): Sequential(
      (0): Sequential(
        (0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): SiLU()
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192)
        (4): SqueezeExcitation(
          (gate): Sequential(
            (0): Reduce('b c h w -> b c', 'mean')
            (1): Linear(in_features=192, out_features=48, bias=False)
            (2): SiLU()
            (3): Linear(in_features=48, out_features=192, bias=False)
            (4): Sigmoid()
            (5): Rearrange('b c -> b c 1 1')
          )
        )
        (5): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
        (6): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): Rearrange('b d (x w1) (y w2) -> b x y w1 w2 d', w1=7, w2=7)
      (2): PreNormResidual(
        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (fn): Attention(
          (to_qkv): Linear(in_features=192, out_features=576, bias=False)
          (attend): Sequential(
            (0): Softmax(dim=-1)
            (1): Dropout(p=0.1, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=192, out_features=192, bias=False)
            (1): Dropout(p=0.1, inplace=False)
          )
          (rel_pos_bias): Embedding(169, 6)
        )
      )
      (3): PreNormResidual(
        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (fn): FeedForward(
          (net): Sequential(
            (0): Linear(in_features=192, out_features=768, bias=True)
            (1): GELU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=768, out_features=192, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): Rearrange('b x y w1 w2 d -> b d (x w1) (y w2)')
      (5): Rearrange('b d (w1 x) (w2 y) -> b x y w1 w2 d', w1=7, w2=7)
      (6): PreNormResidual(
        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (fn): Attention(
          (to_qkv): Linear(in_features=192, out_features=576, bias=False)
          (attend): Sequential(
            (0): Softmax(dim=-1)
            (1): Dropout(p=0.1, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=192, out_features=192, bias=False)
            (1): Dropout(p=0.1, inplace=False)
          )
          (rel_pos_bias): Embedding(169, 6)
        )
      )
      (7): PreNormResidual(
        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (fn): FeedForward(
          (net): Sequential(
            (0): Linear(in_features=192, out_features=768, bias=True)
            (1): GELU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=768, out_features=192, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (8): Rearrange('b x y w1 w2 d -> b d (w1 x) (w2 y)')
    )
    (3): Sequential(
      (0): MBConvResidual(
        (fn): Sequential(
          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU()
          (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
          (4): SqueezeExcitation(
            (gate): Sequential(
              (0): Reduce('b c h w -> b c', 'mean')
              (1): Linear(in_features=192, out_features=48, bias=False)
              (2): SiLU()
              (3): Linear(in_features=48, out_features=192, bias=False)
              (4): Sigmoid()
              (5): Rearrange('b c -> b c 1 1')
            )
          )
          (5): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
          (6): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (dropsample): Dropsample()
      )
      (1): Rearrange('b d (x w1) (y w2) -> b x y w1 w2 d', w1=7, w2=7)
      (2): PreNormResidual(
        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (fn): Attention(
          (to_qkv): Linear(in_features=192, out_features=576, bias=False)
          (attend): Sequential(
            (0): Softmax(dim=-1)
            (1): Dropout(p=0.1, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=192, out_features=192, bias=False)
            (1): Dropout(p=0.1, inplace=False)
          )
          (rel_pos_bias): Embedding(169, 6)
        )
      )
      (3): PreNormResidual(
        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (fn): FeedForward(
          (net): Sequential(
            (0): Linear(in_features=192, out_features=768, bias=True)
            (1): GELU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=768, out_features=192, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): Rearrange('b x y w1 w2 d -> b d (x w1) (y w2)')
      (5): Rearrange('b d (w1 x) (w2 y) -> b x y w1 w2 d', w1=7, w2=7)
      (6): PreNormResidual(
        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (fn): Attention(
          (to_qkv): Linear(in_features=192, out_features=576, bias=False)
          (attend): Sequential(
            (0): Softmax(dim=-1)
            (1): Dropout(p=0.1, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=192, out_features=192, bias=False)
            (1): Dropout(p=0.1, inplace=False)
          )
          (rel_pos_bias): Embedding(169, 6)
        )
      )
      (7): PreNormResidual(
        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (fn): FeedForward(
          (net): Sequential(
            (0): Linear(in_features=192, out_features=768, bias=True)
            (1): GELU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=768, out_features=192, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (8): Rearrange('b x y w1 w2 d -> b d (w1 x) (w2 y)')
    )
    (4): Sequential(
      (0): Sequential(
        (0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): SiLU()
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384)
        (4): SqueezeExcitation(
          (gate): Sequential(
            (0): Reduce('b c h w -> b c', 'mean')
            (1): Linear(in_features=384, out_features=96, bias=False)
            (2): SiLU()
            (3): Linear(in_features=96, out_features=384, bias=False)
            (4): Sigmoid()
            (5): Rearrange('b c -> b c 1 1')
          )
        )
        (5): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
        (6): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): Rearrange('b d (x w1) (y w2) -> b x y w1 w2 d', w1=7, w2=7)
      (2): PreNormResidual(
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (fn): Attention(
          (to_qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attend): Sequential(
            (0): Softmax(dim=-1)
            (1): Dropout(p=0.1, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=False)
            (1): Dropout(p=0.1, inplace=False)
          )
          (rel_pos_bias): Embedding(169, 12)
        )
      )
      (3): PreNormResidual(
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (fn): FeedForward(
          (net): Sequential(
            (0): Linear(in_features=384, out_features=1536, bias=True)
            (1): GELU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=1536, out_features=384, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): Rearrange('b x y w1 w2 d -> b d (x w1) (y w2)')
      (5): Rearrange('b d (w1 x) (w2 y) -> b x y w1 w2 d', w1=7, w2=7)
      (6): PreNormResidual(
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (fn): Attention(
          (to_qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attend): Sequential(
            (0): Softmax(dim=-1)
            (1): Dropout(p=0.1, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=False)
            (1): Dropout(p=0.1, inplace=False)
          )
          (rel_pos_bias): Embedding(169, 12)
        )
      )
      (7): PreNormResidual(
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (fn): FeedForward(
          (net): Sequential(
            (0): Linear(in_features=384, out_features=1536, bias=True)
            (1): GELU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=1536, out_features=384, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (8): Rearrange('b x y w1 w2 d -> b d (w1 x) (w2 y)')
    )
    (5): Sequential(
      (0): MBConvResidual(
        (fn): Sequential(
          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU()
          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (4): SqueezeExcitation(
            (gate): Sequential(
              (0): Reduce('b c h w -> b c', 'mean')
              (1): Linear(in_features=384, out_features=96, bias=False)
              (2): SiLU()
              (3): Linear(in_features=96, out_features=384, bias=False)
              (4): Sigmoid()
              (5): Rearrange('b c -> b c 1 1')
            )
          )
          (5): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
          (6): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (dropsample): Dropsample()
      )
      (1): Rearrange('b d (x w1) (y w2) -> b x y w1 w2 d', w1=7, w2=7)
      (2): PreNormResidual(
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (fn): Attention(
          (to_qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attend): Sequential(
            (0): Softmax(dim=-1)
            (1): Dropout(p=0.1, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=False)
            (1): Dropout(p=0.1, inplace=False)
          )
          (rel_pos_bias): Embedding(169, 12)
        )
      )
      (3): PreNormResidual(
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (fn): FeedForward(
          (net): Sequential(
            (0): Linear(in_features=384, out_features=1536, bias=True)
            (1): GELU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=1536, out_features=384, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): Rearrange('b x y w1 w2 d -> b d (x w1) (y w2)')
      (5): Rearrange('b d (w1 x) (w2 y) -> b x y w1 w2 d', w1=7, w2=7)
      (6): PreNormResidual(
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (fn): Attention(
          (to_qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attend): Sequential(
            (0): Softmax(dim=-1)
            (1): Dropout(p=0.1, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=False)
            (1): Dropout(p=0.1, inplace=False)
          )
          (rel_pos_bias): Embedding(169, 12)
        )
      )
      (7): PreNormResidual(
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (fn): FeedForward(
          (net): Sequential(
            (0): Linear(in_features=384, out_features=1536, bias=True)
            (1): GELU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=1536, out_features=384, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (8): Rearrange('b x y w1 w2 d -> b d (w1 x) (w2 y)')
    )
    (6): Sequential(
      (0): MBConvResidual(
        (fn): Sequential(
          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU()
          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (4): SqueezeExcitation(
            (gate): Sequential(
              (0): Reduce('b c h w -> b c', 'mean')
              (1): Linear(in_features=384, out_features=96, bias=False)
              (2): SiLU()
              (3): Linear(in_features=96, out_features=384, bias=False)
              (4): Sigmoid()
              (5): Rearrange('b c -> b c 1 1')
            )
          )
          (5): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
          (6): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (dropsample): Dropsample()
      )
      (1): Rearrange('b d (x w1) (y w2) -> b x y w1 w2 d', w1=7, w2=7)
      (2): PreNormResidual(
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (fn): Attention(
          (to_qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attend): Sequential(
            (0): Softmax(dim=-1)
            (1): Dropout(p=0.1, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=False)
            (1): Dropout(p=0.1, inplace=False)
          )
          (rel_pos_bias): Embedding(169, 12)
        )
      )
      (3): PreNormResidual(
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (fn): FeedForward(
          (net): Sequential(
            (0): Linear(in_features=384, out_features=1536, bias=True)
            (1): GELU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=1536, out_features=384, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): Rearrange('b x y w1 w2 d -> b d (x w1) (y w2)')
      (5): Rearrange('b d (w1 x) (w2 y) -> b x y w1 w2 d', w1=7, w2=7)
      (6): PreNormResidual(
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (fn): Attention(
          (to_qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attend): Sequential(
            (0): Softmax(dim=-1)
            (1): Dropout(p=0.1, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=False)
            (1): Dropout(p=0.1, inplace=False)
          )
          (rel_pos_bias): Embedding(169, 12)
        )
      )
      (7): PreNormResidual(
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (fn): FeedForward(
          (net): Sequential(
            (0): Linear(in_features=384, out_features=1536, bias=True)
            (1): GELU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=1536, out_features=384, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (8): Rearrange('b x y w1 w2 d -> b d (w1 x) (w2 y)')
    )
    (7): Sequential(
      (0): MBConvResidual(
        (fn): Sequential(
          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU()
          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (4): SqueezeExcitation(
            (gate): Sequential(
              (0): Reduce('b c h w -> b c', 'mean')
              (1): Linear(in_features=384, out_features=96, bias=False)
              (2): SiLU()
              (3): Linear(in_features=96, out_features=384, bias=False)
              (4): Sigmoid()
              (5): Rearrange('b c -> b c 1 1')
            )
          )
          (5): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
          (6): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (dropsample): Dropsample()
      )
      (1): Rearrange('b d (x w1) (y w2) -> b x y w1 w2 d', w1=7, w2=7)
      (2): PreNormResidual(
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (fn): Attention(
          (to_qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attend): Sequential(
            (0): Softmax(dim=-1)
            (1): Dropout(p=0.1, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=False)
            (1): Dropout(p=0.1, inplace=False)
          )
          (rel_pos_bias): Embedding(169, 12)
        )
      )
      (3): PreNormResidual(
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (fn): FeedForward(
          (net): Sequential(
            (0): Linear(in_features=384, out_features=1536, bias=True)
            (1): GELU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=1536, out_features=384, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): Rearrange('b x y w1 w2 d -> b d (x w1) (y w2)')
      (5): Rearrange('b d (w1 x) (w2 y) -> b x y w1 w2 d', w1=7, w2=7)
      (6): PreNormResidual(
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (fn): Attention(
          (to_qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attend): Sequential(
            (0): Softmax(dim=-1)
            (1): Dropout(p=0.1, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=False)
            (1): Dropout(p=0.1, inplace=False)
          )
          (rel_pos_bias): Embedding(169, 12)
        )
      )
      (7): PreNormResidual(
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (fn): FeedForward(
          (net): Sequential(
            (0): Linear(in_features=384, out_features=1536, bias=True)
            (1): GELU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=1536, out_features=384, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (8): Rearrange('b x y w1 w2 d -> b d (w1 x) (w2 y)')
    )
    (8): Sequential(
      (0): MBConvResidual(
        (fn): Sequential(
          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU()
          (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (4): SqueezeExcitation(
            (gate): Sequential(
              (0): Reduce('b c h w -> b c', 'mean')
              (1): Linear(in_features=384, out_features=96, bias=False)
              (2): SiLU()
              (3): Linear(in_features=96, out_features=384, bias=False)
              (4): Sigmoid()
              (5): Rearrange('b c -> b c 1 1')
            )
          )
          (5): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
          (6): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (dropsample): Dropsample()
      )
      (1): Rearrange('b d (x w1) (y w2) -> b x y w1 w2 d', w1=7, w2=7)
      (2): PreNormResidual(
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (fn): Attention(
          (to_qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attend): Sequential(
            (0): Softmax(dim=-1)
            (1): Dropout(p=0.1, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=False)
            (1): Dropout(p=0.1, inplace=False)
          )
          (rel_pos_bias): Embedding(169, 12)
        )
      )
      (3): PreNormResidual(
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (fn): FeedForward(
          (net): Sequential(
            (0): Linear(in_features=384, out_features=1536, bias=True)
            (1): GELU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=1536, out_features=384, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): Rearrange('b x y w1 w2 d -> b d (x w1) (y w2)')
      (5): Rearrange('b d (w1 x) (w2 y) -> b x y w1 w2 d', w1=7, w2=7)
      (6): PreNormResidual(
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (fn): Attention(
          (to_qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attend): Sequential(
            (0): Softmax(dim=-1)
            (1): Dropout(p=0.1, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=False)
            (1): Dropout(p=0.1, inplace=False)
          )
          (rel_pos_bias): Embedding(169, 12)
        )
      )
      (7): PreNormResidual(
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (fn): FeedForward(
          (net): Sequential(
            (0): Linear(in_features=384, out_features=1536, bias=True)
            (1): GELU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=1536, out_features=384, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (8): Rearrange('b x y w1 w2 d -> b d (w1 x) (w2 y)')
    )
    (9): Sequential(
      (0): Sequential(
        (0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): SiLU()
        (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=768)
        (4): SqueezeExcitation(
          (gate): Sequential(
            (0): Reduce('b c h w -> b c', 'mean')
            (1): Linear(in_features=768, out_features=192, bias=False)
            (2): SiLU()
            (3): Linear(in_features=192, out_features=768, bias=False)
            (4): Sigmoid()
            (5): Rearrange('b c -> b c 1 1')
          )
        )
        (5): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
        (6): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): Rearrange('b d (x w1) (y w2) -> b x y w1 w2 d', w1=7, w2=7)
      (2): PreNormResidual(
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fn): Attention(
          (to_qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attend): Sequential(
            (0): Softmax(dim=-1)
            (1): Dropout(p=0.1, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=768, out_features=768, bias=False)
            (1): Dropout(p=0.1, inplace=False)
          )
          (rel_pos_bias): Embedding(169, 24)
        )
      )
      (3): PreNormResidual(
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fn): FeedForward(
          (net): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): Rearrange('b x y w1 w2 d -> b d (x w1) (y w2)')
      (5): Rearrange('b d (w1 x) (w2 y) -> b x y w1 w2 d', w1=7, w2=7)
      (6): PreNormResidual(
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fn): Attention(
          (to_qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attend): Sequential(
            (0): Softmax(dim=-1)
            (1): Dropout(p=0.1, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=768, out_features=768, bias=False)
            (1): Dropout(p=0.1, inplace=False)
          )
          (rel_pos_bias): Embedding(169, 24)
        )
      )
      (7): PreNormResidual(
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fn): FeedForward(
          (net): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (8): Rearrange('b x y w1 w2 d -> b d (w1 x) (w2 y)')
    )
    (10): Sequential(
      (0): MBConvResidual(
        (fn): Sequential(
          (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU()
          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
          (4): SqueezeExcitation(
            (gate): Sequential(
              (0): Reduce('b c h w -> b c', 'mean')
              (1): Linear(in_features=768, out_features=192, bias=False)
              (2): SiLU()
              (3): Linear(in_features=192, out_features=768, bias=False)
              (4): Sigmoid()
              (5): Rearrange('b c -> b c 1 1')
            )
          )
          (5): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
          (6): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (dropsample): Dropsample()
      )
      (1): Rearrange('b d (x w1) (y w2) -> b x y w1 w2 d', w1=7, w2=7)
      (2): PreNormResidual(
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fn): Attention(
          (to_qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attend): Sequential(
            (0): Softmax(dim=-1)
            (1): Dropout(p=0.1, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=768, out_features=768, bias=False)
            (1): Dropout(p=0.1, inplace=False)
          )
          (rel_pos_bias): Embedding(169, 24)
        )
      )
      (3): PreNormResidual(
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fn): FeedForward(
          (net): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): Rearrange('b x y w1 w2 d -> b d (x w1) (y w2)')
      (5): Rearrange('b d (w1 x) (w2 y) -> b x y w1 w2 d', w1=7, w2=7)
      (6): PreNormResidual(
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fn): Attention(
          (to_qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attend): Sequential(
            (0): Softmax(dim=-1)
            (1): Dropout(p=0.1, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=768, out_features=768, bias=False)
            (1): Dropout(p=0.1, inplace=False)
          )
          (rel_pos_bias): Embedding(169, 24)
        )
      )
      (7): PreNormResidual(
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fn): FeedForward(
          (net): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (8): Rearrange('b x y w1 w2 d -> b d (w1 x) (w2 y)')
    )
  )
  (mlp_head): Sequential(
    (0): Reduce('b d h w -> b d', 'mean')
    (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (2): Linear(in_features=768, out_features=1000, bias=True)
  )
)