model

CeiT(
  (conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(7, 7), stride=(2, 2), padding=(4, 4))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (to_patch_embedding): Sequential(
    (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=4, p2=4)
    (1): Linear(in_features=512, out_features=192, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (transformer): TransformerLeFF(
    (layers): ModuleList(
      (0): ModuleList(
        (0): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=192, out_features=576, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=192, out_features=192, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): LeFF(
              (up_proj): Sequential(
                (0): Linear(in_features=192, out_features=768, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c (h w) -> b c h w', h=14, w=14)
              )
              (depth_conv): Sequential(
                (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
                (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): GELU()
                (3): Rearrange('b c h w -> b (h w) c', h=14, w=14)
              )
              (down_proj): Sequential(
                (0): Linear(in_features=768, out_features=192, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c n -> b n c')
              )
            )
          )
        )
      )
      (1): ModuleList(
        (0): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=192, out_features=576, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=192, out_features=192, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): LeFF(
              (up_proj): Sequential(
                (0): Linear(in_features=192, out_features=768, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c (h w) -> b c h w', h=14, w=14)
              )
              (depth_conv): Sequential(
                (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
                (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): GELU()
                (3): Rearrange('b c h w -> b (h w) c', h=14, w=14)
              )
              (down_proj): Sequential(
                (0): Linear(in_features=768, out_features=192, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c n -> b n c')
              )
            )
          )
        )
      )
      (2): ModuleList(
        (0): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=192, out_features=576, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=192, out_features=192, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): LeFF(
              (up_proj): Sequential(
                (0): Linear(in_features=192, out_features=768, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c (h w) -> b c h w', h=14, w=14)
              )
              (depth_conv): Sequential(
                (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
                (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): GELU()
                (3): Rearrange('b c h w -> b (h w) c', h=14, w=14)
              )
              (down_proj): Sequential(
                (0): Linear(in_features=768, out_features=192, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c n -> b n c')
              )
            )
          )
        )
      )
      (3): ModuleList(
        (0): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=192, out_features=576, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=192, out_features=192, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): LeFF(
              (up_proj): Sequential(
                (0): Linear(in_features=192, out_features=768, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c (h w) -> b c h w', h=14, w=14)
              )
              (depth_conv): Sequential(
                (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
                (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): GELU()
                (3): Rearrange('b c h w -> b (h w) c', h=14, w=14)
              )
              (down_proj): Sequential(
                (0): Linear(in_features=768, out_features=192, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c n -> b n c')
              )
            )
          )
        )
      )
      (4): ModuleList(
        (0): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=192, out_features=576, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=192, out_features=192, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): LeFF(
              (up_proj): Sequential(
                (0): Linear(in_features=192, out_features=768, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c (h w) -> b c h w', h=14, w=14)
              )
              (depth_conv): Sequential(
                (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
                (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): GELU()
                (3): Rearrange('b c h w -> b (h w) c', h=14, w=14)
              )
              (down_proj): Sequential(
                (0): Linear(in_features=768, out_features=192, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c n -> b n c')
              )
            )
          )
        )
      )
      (5): ModuleList(
        (0): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=192, out_features=576, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=192, out_features=192, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): LeFF(
              (up_proj): Sequential(
                (0): Linear(in_features=192, out_features=768, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c (h w) -> b c h w', h=14, w=14)
              )
              (depth_conv): Sequential(
                (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
                (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): GELU()
                (3): Rearrange('b c h w -> b (h w) c', h=14, w=14)
              )
              (down_proj): Sequential(
                (0): Linear(in_features=768, out_features=192, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c n -> b n c')
              )
            )
          )
        )
      )
      (6): ModuleList(
        (0): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=192, out_features=576, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=192, out_features=192, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): LeFF(
              (up_proj): Sequential(
                (0): Linear(in_features=192, out_features=768, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c (h w) -> b c h w', h=14, w=14)
              )
              (depth_conv): Sequential(
                (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
                (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): GELU()
                (3): Rearrange('b c h w -> b (h w) c', h=14, w=14)
              )
              (down_proj): Sequential(
                (0): Linear(in_features=768, out_features=192, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c n -> b n c')
              )
            )
          )
        )
      )
      (7): ModuleList(
        (0): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=192, out_features=576, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=192, out_features=192, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): LeFF(
              (up_proj): Sequential(
                (0): Linear(in_features=192, out_features=768, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c (h w) -> b c h w', h=14, w=14)
              )
              (depth_conv): Sequential(
                (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
                (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): GELU()
                (3): Rearrange('b c h w -> b (h w) c', h=14, w=14)
              )
              (down_proj): Sequential(
                (0): Linear(in_features=768, out_features=192, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c n -> b n c')
              )
            )
          )
        )
      )
      (8): ModuleList(
        (0): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=192, out_features=576, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=192, out_features=192, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): LeFF(
              (up_proj): Sequential(
                (0): Linear(in_features=192, out_features=768, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c (h w) -> b c h w', h=14, w=14)
              )
              (depth_conv): Sequential(
                (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
                (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): GELU()
                (3): Rearrange('b c h w -> b (h w) c', h=14, w=14)
              )
              (down_proj): Sequential(
                (0): Linear(in_features=768, out_features=192, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c n -> b n c')
              )
            )
          )
        )
      )
      (9): ModuleList(
        (0): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=192, out_features=576, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=192, out_features=192, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): LeFF(
              (up_proj): Sequential(
                (0): Linear(in_features=192, out_features=768, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c (h w) -> b c h w', h=14, w=14)
              )
              (depth_conv): Sequential(
                (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
                (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): GELU()
                (3): Rearrange('b c h w -> b (h w) c', h=14, w=14)
              )
              (down_proj): Sequential(
                (0): Linear(in_features=768, out_features=192, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c n -> b n c')
              )
            )
          )
        )
      )
      (10): ModuleList(
        (0): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=192, out_features=576, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=192, out_features=192, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): LeFF(
              (up_proj): Sequential(
                (0): Linear(in_features=192, out_features=768, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c (h w) -> b c h w', h=14, w=14)
              )
              (depth_conv): Sequential(
                (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
                (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): GELU()
                (3): Rearrange('b c h w -> b (h w) c', h=14, w=14)
              )
              (down_proj): Sequential(
                (0): Linear(in_features=768, out_features=192, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c n -> b n c')
              )
            )
          )
        )
      )
      (11): ModuleList(
        (0): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=192, out_features=576, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=192, out_features=192, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (fn): LeFF(
              (up_proj): Sequential(
                (0): Linear(in_features=192, out_features=768, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c (h w) -> b c h w', h=14, w=14)
              )
              (depth_conv): Sequential(
                (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
                (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): GELU()
                (3): Rearrange('b c h w -> b (h w) c', h=14, w=14)
              )
              (down_proj): Sequential(
                (0): Linear(in_features=768, out_features=192, bias=True)
                (1): Rearrange('b n c -> b c n')
                (2): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (3): GELU()
                (4): Rearrange('b c n -> b n c')
              )
            )
          )
        )
      )
    )
  )
  (LCA): LCA(
    (layers): ModuleList(
      (0): ModuleList(
        (0): PreNorm(
          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (fn): LCAttention(
            (to_qkv): Linear(in_features=192, out_features=576, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=192, out_features=192, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): PreNorm(
          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (fn): FeedForward(
            (net): Sequential(
              (0): Linear(in_features=192, out_features=384, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=384, out_features=192, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
  )
  (to_latent): Identity()
  (mlp_head): Sequential(
    (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=192, out_features=10, bias=True)
  )
)
